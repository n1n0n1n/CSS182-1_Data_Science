

PyTest.ipynb
PyTest.ipynb_

[ ]
!pip install nltk
import pandas as pd
import numpy as np
Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)
Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)
Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)
Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)
Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)
First load pandas library for working with datasets. Setting the pandas library as "pd" for short and ease of use


[ ]
from google.colab import files
uploaded = files.upload()

Using the code to import a file. and uploading it to google colab to use the data set.


[ ]
df = pd.read_csv('twitter_stock_market.csv', sep=';', encoding='utf-8')
Making pandas read the csv dataset. And abbreviating it to "df" in short for dataframe for ease of use.


[ ]
df.head()

Using df.head() to call 5 first lines of row. And to show what the 5 first row of data looks like.


[ ]
df.shape
(5000, 4)
Using df.shape to show how many rows and columns.


[ ]
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5000 entries, 0 to 4999
Data columns (total 4 columns):
 #   Column      Non-Null Count  Dtype 
---  ------      --------------  ----- 
 0   id          5000 non-null   int64 
 1   created_at  5000 non-null   object
 2   text        5000 non-null   object
 3   sentiment   1300 non-null   object
dtypes: int64(1), object(3)
memory usage: 156.4+ KB
Using df.info() to show how many entries per column. And showing what type of data is accepted.


[ ]
df.columns
Index(['id', 'created_at', 'text', 'sentiment'], dtype='object')
Using df.columns to show the "headings" of each entry for the rows. Showing the title of each column of what is important for the analysis, which in this case the text and sentiment column

Downloaded from Kaggle: Twitter Stock Market Sentiment Dataset The dataset includes 5000 rows of data samples, where the dataset is sourced from tweets of the old social media "Twitter", and the categories of the sentiments are the usual "positive, negative, and neutral"

References https://colab.research.google.com/drive/1LZMiI5nr7BP68kdLBo6yjBo5bT5OpXVQ?authuser=3#scrollTo=Q5cyxU8oipSl

https://stackoverflow.com/questions/47430544/load-xlsx-file-from-drive-in-colaboratory

https://www.w3schools.com/python/pandas/pandas_csv.asp

https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?resource=download

https://www.kaggle.com/datasets/utkarshxy/stock-markettweets-lexicon-data


[ ]

Start coding or generate with AI.
Colab paid products - Cancel contracts here


!pip install nltk
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import nltk
from nltk.corpus import stopwords
import string

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab') # Download punkt_tab resource

*/
Installing nltk (Natural Language Toolkit Library), Providing tools for processing for tokenization, stemming, stopwords, etc.

Importing Pandas library. used for data manipulation and analysis.

Importing NumPy library. used for numerical operations in Python.

Importing function to split datasets into training and testing sets. Helps to evaluate model for unseen data.

Importing TF-IDF vectorizer. That converts text data into numerical features, based on frequency and inverse document frequency.

Imports Logistic Regression Model. used for binary and multiclass classification tasks.

Importing evaluation metrics "accuracy_score" and "classification_report". Giving the percentage of correct predictions and shows precision, recall, and F1-score.

Importing NLTK module to work with human language data. Used for tokenization, stemming, and removing stopwords.

Importing the list of common English stopwords from NLTK. Stop Words are often removed to focus on meaningful words in text.

Imports Python's string module. Includes a list of punctuation characters, used to clean text by removing punctuations.

Downloads stopwords dataset required by NLTK. Ensuring code can use stopwords.words() without error.

Downloads punkt tokenizer models for sentence and word tokenization. Allows functions to work properly.
/*


df = pd.read_csv('twitter_stock_market.csv', sep=';', encoding='utf-8') #changed to previous dataset file name for consistency


df.head()         # First few rows

#Shows first few rows of data. Showing the first 5 rows of data as default to get a grasp of the dataset.


df.shape

#Shows the Rows, Column of the data set. Showing the data set has 5000 rows of data and 4 columns for categories.


df.info()

#Shows the information of the data set including the columns and the entries of how many rows of data per column. This shows how many row entries of data per column is included.


df.columns

#Showing the columns of the data set. Using the code for identifying what is needed or significant for sentiment analysis, which is "text" and "sentiment" columns.



stop_words = set(stopwords.words('english'))
punctuations = string.punctuation

# Preprocessing function
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = ''.join([char for char in text if char not in punctuations])
    # Remove stopwords
    words = nltk.word_tokenize(text)
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

# Apply to the text column
df['clean_text'] = df['text'].astype(str).apply(preprocess_text)

# Check result
df[['text', 'clean_text']].head()



!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

*/
First line of code installs VADER Sentiment Analysis library. Using this to analyze the sentiment of text.

Second line imports the "SentimentIntensityAnalyzer" class from vaderSentiment. Using the code to compute sentiment scores (e.g. positive, negative,neutral) from a given set of text.
/*


from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
# Define sentiment scoring function
def get_sentiment(text):
  return analyzer.polarity_scores(text)
# Apply sentiment analysis
sentiment_scores = df['clean_text'].apply(get_sentiment)
# Convert dictionary output to columns
df_sentiments = pd.DataFrame(sentiment_scores.tolist())
# Merge back into the main DataFrame
df = pd.concat([df, df_sentiments], axis=1)
# Preview the result
df[['clean_text', 'compound', 'neg', 'neu', 'pos']].head()


*/
First line of code imports SentimentIntensityAnalyzer class from vaderSentiment. Using this code calculates sentiment scores from text.

Second line of code creates an instance of the sentiment analyzer to use and analyze multiple text inputs. Using this returns sentiment scores for a given string.

Third line of code defines sentiment scoring function. Using this takes the text string and returns a dictionary of sentiment scores using VADER.

Fourth line of code applies sentiment analysis, get_sentiment() function is applied to each row in clean_text column of the data frame "df". Using this code results in a new series of dictionaries containing sentiment scores.

Fifth line of code combines df and df_sentiments data frame side by side. Using this attaches the sentiment scores to the original data.

Sixth line of code displays first few rows of the columns. Using this shows the first few rows of the data frame that shows cleaned text and associated sentiment scores.
/*


nltk.download('vader_lexicon')
# Initialize VADER sentiment analyzer
sid = SentimentIntensityAnalyzer()
# Calculate the compound sentiment score for each review
df['compound'] = df['clean_text'].apply(lambda x: sid.polarity_scores(x)['compound'])
# Define the sentiment classification function
def classify_sentiment(score):
  if score >= 0.05:
    return 'Positive'
  elif score <= -0.05:
    return 'Negative'
  else:
    return 'Neutral'
# Apply the classification to create the sentiment label column
df['Sentiment_Label'] = df['compound'].apply(classify_sentiment)
# Display the sentiment analysis table
print(df[['clean_text', 'compound', 'Sentiment_Label']].head())


*/
First line downloads the VADER lexicon needed for sentiment analysis.

Second, creates a VADER sentiment analyzer object called sid.

Third line calculates a sentiment score called “compound” for each cleaned review in the DataFrame and adds it as a new column.

Defines a function that labels the sentiment as “Positive” if the score is 0.05 or higher, “Negative” if the score is -0.05 or lower, and “Neutral” if it’s in between.

Applies this function to the compound scores to create a new column with sentiment labels.

Finally, prints the first few rows of the DataFrame showing the review text, its compound score, and its sentiment label.
/*


import seaborn as sns
import matplotlib.pyplot as plt
# Plot the sentiment distribution using Seaborn
sns.countplot(x='Sentiment_Label', data=df, palette='Set2')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.show()


*/
The first line of code brings in the Seaborn library, which helps make nice and easy charts.

The second line brings in Matplotlib’s pyplot, another tool to help draw and show graphs.

The third line draws a bar chart that counts how many reviews there are for each sentiment (like positive, negative) using the data in the Sentiment_Label column.

The fourth line adds a title at the top of the chart that says “Sentiment Distribution” so we know what the chart is about.

The fifth line labels the bottom of the chart as “Sentiment” to show what the categories mean.

The sixth line labels the side of the chart as “Number of Reviews” to show how many reviews are in each sentiment.
/*


## Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

X = df['clean_text']
y = df['Sentiment_Label']
X_train, X_test, y_train, y_test = train_test_split(
  X, y,
  test_size=0.2,
  random_state=42,
  stratify=y
)

print(f"Training set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")


*/
The first line of code imports the train_test_split function from Scikit-learn, which is used to divide the dataset into training and testing parts.

The second line of code assigns the column 'clean_text' from the DataFrame df to the variable X, representing the input features.

The third line of code assigns the column 'Sentiment_Label' from the DataFrame df to the variable y, representing the target sentiment labels.

The fourth line of code, along with the following lines, uses train_test_split to split X and y into training and testing sets; it allocates 20% of the data for testing, uses a random seed of 42 for reproducibility, and applies stratified sampling to keep the same class distribution in both sets.

The last two lines of code print out the number of samples in the training set (X_train) and in the testing set (X_test).
/*


# Fit the vectorizer on the training data and transform it into a TF-IDF feature matrix
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=5000)

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

X_test_tfidf = tfidf_vectorizer.transform(X_test)

print(f"TF-IDF feature matrix shape: {X_train_tfidf.shape}")


*/
First line of code imports the TfidfVectorizer class from Scikit-learn, which is used to convert a collection of raw text documents into a matrix of TF-IDF features.

The second line of code initializes the TF-IDF vectorizer, limiting it to the top 5000 most significant words in the dataset. This helps reduce noise and computation time by focusing only on the most relevant terms.

the third line fits the vectorizer to the training text data (learning the vocabulary and calculating IDF), and then transforms the training text into a sparse TF-IDF matrix.

The fourth line transforms the test text data using the same vocabulary and IDF values learned from the training set, ensuring consistency between training and testing inputs.

The last line prints the shape of the resulting TF-IDF matrix, showing the number of training samples and the number of extracted features (up to 5000).

Used TfidfVectorizer from Scikit-learn to convert training and test text data into numerical TF-IDF feature matrices, limiting to the top 5000 words for efficiency and consistency between datasets.
/*

## Choose and Train a Model (Logistic Regression)

from sklearn.linear_model import LogisticRegression

# Initialize the Logistic Regression model
# 'max_iter=1000' ensures the solver has enough iterations to converge
# 'random_state=42' ensures reproducibility of results
lr_model = LogisticRegression(max_iter=1000, random_state=42)

# Train (fit) the Logistic Regression model using the training TF-IDF features and labels
lr_model.fit(X_train_tfidf, y_train)

*/
First line of code imports LogisticRegression class from scikit-learn's linear model module. Using this for classification tasks.

Second line of code creates a Logistic Regression model instance. Using this to allow the algorithm by only including 1000 iterations for converging (preventing warnings), and sets a seed for the model training results to be reproducible every time the code runs.

Third line of code trains the model. Using this code contains the TF-IDF tranformed text features, and contains the true class labels, which the model learns patterns to relate text inputs to corresponding labels.

A Logistic Regression model is initialized and trained using the TF-IDF-transformed training data. The model learns to associate word patterns with sentiment labels, enabling it to make predictions on unseen data.
/*


## Make Predictions and Evaluate the Model

from sklearn.metrics import accuracy_score, classification_report

# Generate predictions on the test set using the trained Logistic Regression model
y_pred = lr_model.predict(X_test_tfidf)

# Calculate and display the accuracy of the model
# Accuracy = (Number of correct predictions) / (Total predictions)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Display a detailed classification report
# Includes precision, recall, F1-score, and support for each class
print("\nClassification Report:")
print(classification_report(y_test, y_pred))


*/
First line of code imports two evaluation functions from scikit-learn. Using this to calculate overall prediction accuracy, and giving detailed metrics like precision, recall, F1-score, and etc.

Second line of code uses trained Logistic Regression model to predict labels for test feature set. Using this to contain the predicted class for each test sample.

Third line of code calculates accuracy score by coomparing the two test and predicted labels. Using this to show the accuracy score of the comparison. And prints the accuracy score.

Fourth line of code prints the Classification Report. And the results for the precision, recall, F1- scores, and support.

The trained model is tested on the unseen test set. Predictions are evaluated using accuracy and a classification report (precision, recall, F1-score) to assess how well the model distinguishes between sentiment classes.
/*

